<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Distributed Hate Machine</title>
    <link>//localhost:1313/</link>
    <description>Recent content in Home on Distributed Hate Machine</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 19 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reviving Jetson Nano to run some LLMs</title>
      <link>//localhost:1313/posts/jetson/</link>
      <pubDate>Sun, 19 Oct 2025 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/jetson/</guid>
      <description>Large Language Models—minus the Large—on a tiny Jetson Nano.</description>
    </item>
    <item>
      <title>About me</title>
      <link>//localhost:1313/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/about/</guid>
      <description>&lt;p&gt;applied math undergrad&lt;br&gt;&#xA;pls hire, cv upon request (im bit shy)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Today I Learned</title>
      <link>//localhost:1313/til/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/til/</guid>
      <description>&lt;h2 class=&#34;heading&#34; id=&#34;2025-10-19&#34;&gt;&#xA;  2025-10-19&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2025-10-19&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SlkEW4C2kd4&#34;&gt;How containers work in Modal&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://modal.com/llm-almanac/how-to-benchmark&#34;&gt;Benchmarking inference engines&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 class=&#34;heading&#34; id=&#34;2025-10-15&#34;&gt;&#xA;  2025-10-15&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2025-10-15&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.aleksagordic.com/blog/vllm&#34;&gt;Digging in vLLM&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.aleksagordic.com/blog/matmul&#34;&gt;How highperf matmul kernels work&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 class=&#34;heading&#34; id=&#34;2025-10-13&#34;&gt;&#xA;  2025-10-13&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2025-10-13&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.thonking.ai/p/strangely-matrix-multiplications&#34;&gt;Matmuls are weird&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://modal.com/blog/reverse-engineer-flash-attention-4&#34;&gt;Flash Attention 4&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=uZTtViomW6w&#34;&gt;Great talk on Nvidia Cutile&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf&#34;&gt;Nvidia Tesla paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 class=&#34;heading&#34; id=&#34;2025-10-12&#34;&gt;&#xA;  2025-10-12&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2025-10-12&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://cognition.ai/blog/kevin-32b&#34;&gt;LLM for kernels&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://scalingintelligence.stanford.edu/blogs/kernelbench/&#34;&gt;KernelBench&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai/&#34;&gt;Accelerating GenAI part 1 / SAM Fast&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/meta-pytorch/segment-anything-fast/tree/main&#34;&gt;SAM Fast code&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai/&#34;&gt;Accelerating GenAI part 2 / GPT Fast&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai/&#34;&gt;GPT Fast Code&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/exploiting-ampere-structured-sparsity-with-cusparselt&#34;&gt;Way to faster matmul using sparsification&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://dev-discuss.pytorch.org/t/min-cut-optimal-recomputation-i-e-activation-checkpointing-with-aotautograd/467&#34;&gt;Recomputation using nerdy algos&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2007.00072&#34;&gt;Paper on optimizing data movement in transformers&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://dev-discuss.pytorch.org/t/torchdynamo-an-experiment-in-dynamic-python-bytecode-transformation/361&#34;&gt;TorchDynamo&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/flexattention/&#34;&gt;FlexAttention&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.thonking.ai/p/short-supporting-mixtral-in-gpt-fast&#34;&gt;Supporting Mixtral in GPT Fast&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.thonking.ai/p/what-shapes-do-matrix-multiplications&#34;&gt;What Shapes Do Matrix Multiplications Like?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 class=&#34;heading&#34; id=&#34;2025-10-11&#34;&gt;&#xA;  2025-10-11&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2025-10-11&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 class=&#34;heading&#34; id=&#34;lora-serving&#34;&gt;&#xA;  Lora serving&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#lora-serving&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/peft/package_reference/hotswap&#34;&gt;Dynamically swap LoRA adapters in PEFT&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.vllm.ai/en/stable/features/lora.html#dynamically-serving-lora-adapters&#34;&gt;vLLM LoRA Serving&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/multi-lora-serving&#34;&gt;Multi-LoRA Serving from HF TGI&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/predibase/lorax&#34;&gt;Multi-LoRA inference server from Predibase&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 class=&#34;heading&#34; id=&#34;uncategorized&#34;&gt;&#xA;  Uncategorized&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#uncategorized&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1502.05767&#34;&gt;Great paper on Automatic Differentiation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://distributedlexicon.com/&#34;&gt;Distributed Training Lexicon&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://allmodelsarewrong.github.io/&#34;&gt;All Models Are Wrong: Concepts of Statistical Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://epic-guide.github.io/applying&#34;&gt;Epic guide on writing CV, statement of purpose, letters of recommendation, specifically for applying to EPFL&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://fleetwood.dev/posts/domain-specific-architectures&#34;&gt;Domain Specific Architectures&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
